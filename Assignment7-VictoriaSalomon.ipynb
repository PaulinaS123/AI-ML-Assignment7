{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b1775e-fb37-42e2-9b83-ba65961b7a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (4.4.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: peft in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/hf/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets scikit-learn peft torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb0ec5-fedb-4cd5-a612-5ace9a830e72",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628d852d-cda1-489a-b933-c5aa3620962e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d4822a1444482181cb47be7baf9020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove raw text columns\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2e12b-b018-451d-8b9c-cb74a3c1c208",
   "metadata": {},
   "source": [
    "## Create Data Loaders and PEFT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02924d31-2660-456a-baaa-c9f2138e5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "train_dataset = tokenized_datasets['train']\n",
    "test_dataset = tokenized_datasets['test']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48cfda31-393b-4a5b-8ece-5a2242f70c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Load base model\n",
    "model_name = \"distilbert-base-uncased\"  # Added model_name definition\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Print model structure to identify correct module names\n",
    "# Uncomment this to debug the model structure\n",
    "# for name, module in base_model.named_modules():\n",
    "#     print(name)\n",
    "\n",
    "# Configure LoRA with corrected target modules\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    # Corrected target modules to match DistilBERT's actual architecture\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"],  # Simplified pattern that matches DistilBERT's attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "# Inject LoRA\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30faa793-4094-405e-8f86-6c35548ebf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distilbert\n",
      "distilbert.embeddings\n",
      "distilbert.embeddings.word_embeddings\n",
      "distilbert.embeddings.position_embeddings\n",
      "distilbert.embeddings.LayerNorm\n",
      "distilbert.embeddings.dropout\n",
      "distilbert.transformer\n",
      "distilbert.transformer.layer\n",
      "distilbert.transformer.layer.0\n",
      "distilbert.transformer.layer.0.attention\n",
      "distilbert.transformer.layer.0.attention.dropout\n",
      "distilbert.transformer.layer.0.attention.q_lin\n",
      "distilbert.transformer.layer.0.attention.k_lin\n",
      "distilbert.transformer.layer.0.attention.v_lin\n",
      "distilbert.transformer.layer.0.attention.out_lin\n",
      "distilbert.transformer.layer.0.sa_layer_norm\n",
      "distilbert.transformer.layer.0.ffn\n",
      "distilbert.transformer.layer.0.ffn.dropout\n",
      "distilbert.transformer.layer.0.ffn.lin1\n",
      "distilbert.transformer.layer.0.ffn.lin2\n",
      "distilbert.transformer.layer.0.ffn.activation\n",
      "distilbert.transformer.layer.0.output_layer_norm\n",
      "distilbert.transformer.layer.1\n",
      "distilbert.transformer.layer.1.attention\n",
      "distilbert.transformer.layer.1.attention.dropout\n",
      "distilbert.transformer.layer.1.attention.q_lin\n",
      "distilbert.transformer.layer.1.attention.k_lin\n",
      "distilbert.transformer.layer.1.attention.v_lin\n",
      "distilbert.transformer.layer.1.attention.out_lin\n",
      "distilbert.transformer.layer.1.sa_layer_norm\n",
      "distilbert.transformer.layer.1.ffn\n",
      "distilbert.transformer.layer.1.ffn.dropout\n",
      "distilbert.transformer.layer.1.ffn.lin1\n",
      "distilbert.transformer.layer.1.ffn.lin2\n",
      "distilbert.transformer.layer.1.ffn.activation\n",
      "distilbert.transformer.layer.1.output_layer_norm\n",
      "distilbert.transformer.layer.2\n",
      "distilbert.transformer.layer.2.attention\n",
      "distilbert.transformer.layer.2.attention.dropout\n",
      "distilbert.transformer.layer.2.attention.q_lin\n",
      "distilbert.transformer.layer.2.attention.k_lin\n",
      "distilbert.transformer.layer.2.attention.v_lin\n",
      "distilbert.transformer.layer.2.attention.out_lin\n",
      "distilbert.transformer.layer.2.sa_layer_norm\n",
      "distilbert.transformer.layer.2.ffn\n",
      "distilbert.transformer.layer.2.ffn.dropout\n",
      "distilbert.transformer.layer.2.ffn.lin1\n",
      "distilbert.transformer.layer.2.ffn.lin2\n",
      "distilbert.transformer.layer.2.ffn.activation\n",
      "distilbert.transformer.layer.2.output_layer_norm\n",
      "distilbert.transformer.layer.3\n",
      "distilbert.transformer.layer.3.attention\n",
      "distilbert.transformer.layer.3.attention.dropout\n",
      "distilbert.transformer.layer.3.attention.q_lin\n",
      "distilbert.transformer.layer.3.attention.k_lin\n",
      "distilbert.transformer.layer.3.attention.v_lin\n",
      "distilbert.transformer.layer.3.attention.out_lin\n",
      "distilbert.transformer.layer.3.sa_layer_norm\n",
      "distilbert.transformer.layer.3.ffn\n",
      "distilbert.transformer.layer.3.ffn.dropout\n",
      "distilbert.transformer.layer.3.ffn.lin1\n",
      "distilbert.transformer.layer.3.ffn.lin2\n",
      "distilbert.transformer.layer.3.ffn.activation\n",
      "distilbert.transformer.layer.3.output_layer_norm\n",
      "distilbert.transformer.layer.4\n",
      "distilbert.transformer.layer.4.attention\n",
      "distilbert.transformer.layer.4.attention.dropout\n",
      "distilbert.transformer.layer.4.attention.q_lin\n",
      "distilbert.transformer.layer.4.attention.k_lin\n",
      "distilbert.transformer.layer.4.attention.v_lin\n",
      "distilbert.transformer.layer.4.attention.out_lin\n",
      "distilbert.transformer.layer.4.sa_layer_norm\n",
      "distilbert.transformer.layer.4.ffn\n",
      "distilbert.transformer.layer.4.ffn.dropout\n",
      "distilbert.transformer.layer.4.ffn.lin1\n",
      "distilbert.transformer.layer.4.ffn.lin2\n",
      "distilbert.transformer.layer.4.ffn.activation\n",
      "distilbert.transformer.layer.4.output_layer_norm\n",
      "distilbert.transformer.layer.5\n",
      "distilbert.transformer.layer.5.attention\n",
      "distilbert.transformer.layer.5.attention.dropout\n",
      "distilbert.transformer.layer.5.attention.q_lin\n",
      "distilbert.transformer.layer.5.attention.k_lin\n",
      "distilbert.transformer.layer.5.attention.v_lin\n",
      "distilbert.transformer.layer.5.attention.out_lin\n",
      "distilbert.transformer.layer.5.sa_layer_norm\n",
      "distilbert.transformer.layer.5.ffn\n",
      "distilbert.transformer.layer.5.ffn.dropout\n",
      "distilbert.transformer.layer.5.ffn.lin1\n",
      "distilbert.transformer.layer.5.ffn.lin2\n",
      "distilbert.transformer.layer.5.ffn.activation\n",
      "distilbert.transformer.layer.5.output_layer_norm\n",
      "pre_classifier\n",
      "classifier\n",
      "dropout\n"
     ]
    }
   ],
   "source": [
    "for name, module in base_model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "001ba328-955e-460c-98d6-b5b541a65eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffa6ea1d-50c5-463e-8848-6257aed9dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare encodings for train and test datasets\n",
    "train_texts = list(dataset['train']['text'])\n",
    "train_labels = list(dataset['train']['label'])\n",
    "\n",
    "test_texts = list(dataset['test']['text'])\n",
    "test_labels = list(dataset['test']['label'])\n",
    "\n",
    "# Tokenize\n",
    "train_encodings = tokenizer(train_texts, padding=True, truncation=True, max_length=256)\n",
    "test_encodings = tokenizer(test_texts, padding=True, truncation=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afb39481-29ce-446c-b7b7-dbe286cf4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert each item explicitly to numpy array or list\n",
    "        item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ed044-d390-4f5b-afd3-5b457e4df4fe",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2bf25f45-ab3a-4475-9452-80501e453d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lists\n",
    "train_texts = list(dataset['train']['text'])[:10]\n",
    "train_labels = list(dataset['train']['label'])[:10]\n",
    "\n",
    "test_texts = list(dataset['test']['text'])[:10]\n",
    "test_labels = list(dataset['test']['label'])[:10]\n",
    "\n",
    "# Tokenize\n",
    "train_encodings = tokenizer(train_texts, padding=True, truncation=True, max_length=256)\n",
    "test_encodings = tokenizer(test_texts, padding=True, truncation=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad9f5ca3-7ad4-4866-9304-5d618b09033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cce37fa-78d7-4ec7-b56c-0558d99f330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fec02a-2ee4-4bd2-93ac-1581d2d9dac8",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef054b36-51ee-4a45-95ee-dc7ecc2d71bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/1 starting\n",
      "Epoch 1 finished. Loss: 1.7837, Accuracy: 0.9000, F1 Score: 0.9474\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs} starting\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished. Loss: {total_loss:.4f}, Accuracy: {epoch_acc:.4f}, F1 Score: {epoch_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c280d3a4-6315-435a-ac2d-820c59d909d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've already tokenized using tokenizer\n",
    "encodings = tokenizer(list(dataset['train']['text']), padding=True, truncation=True, max_length=256)\n",
    "labels = list(dataset['train']['label'])\n",
    "\n",
    "train_dataset = IMDbDataset(encodings, labels)\n",
    "\n",
    "# For test\n",
    "encodings_test = tokenizer(list(dataset['test']['text']), padding=True, truncation=True, max_length=256)\n",
    "labels_test = list(dataset['test']['label'])\n",
    "\n",
    "test_dataset = IMDbDataset(encodings_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4cc0c66-a54a-4f94-aebd-bd24b0c17fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'lora_distilbert_sentiment/'\n"
     ]
    }
   ],
   "source": [
    "# Save the model with LoRA adapters\n",
    "model.save_pretrained(\"lora_distilbert_sentiment\")\n",
    "tokenizer.save_pretrained(\"lora_distilbert_sentiment\")\n",
    "print(\"Model saved to 'lora_distilbert_sentiment/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa54b5-880f-4a15-919f-1e92ed485e1d",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a356eb97-7440-4d21-a37d-f987b67943ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'This movie was fantastic! I loved it.'\n",
      "Prediction: Positive (Confidence: 0.83)\n",
      "\n",
      "Text: 'The film was boring and too long.'\n",
      "Prediction: Negative (Confidence: 0.94)\n",
      "\n",
      "Text: 'An average movie, nothing special.'\n",
      "Prediction: Negative (Confidence: 0.93)\n",
      "\n",
      "Text: 'I really enjoyed the story and acting.'\n",
      "Prediction: Positive (Confidence: 0.89)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        pred_label = torch.argmax(probs, dim=-1).cpu().item()\n",
    "        confidence = probs.max().cpu().item()\n",
    "    label_str = \"Positive\" if pred_label == 1 else \"Negative\"\n",
    "    return label_str, confidence\n",
    "\n",
    "# Sample texts\n",
    "texts = [\n",
    "    \"This movie was fantastic! I loved it.\",\n",
    "    \"The film was boring and too long.\",\n",
    "    \"An average movie, nothing special.\",\n",
    "    \"I really enjoyed the story and acting.\"\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    label, conf = predict_text(t)\n",
    "    print(f\"Text: '{t}'\\nPrediction: {label} (Confidence: {conf:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ce665-1229-4c74-b167-461134af9b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load your model for inference\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lora_distilbert_sentiment\")\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model = PeftModel.from_pretrained(base_model, \"lora_distilbert_sentiment\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Prepare data\n",
    "labels_true = list(dataset['test']['label'])\n",
    "predicted_labels = []\n",
    "\n",
    "for text in dataset['test']['text']:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        pred_label = torch.argmax(probs, dim=-1).cpu().item()\n",
    "        predicted_labels.append(pred_label)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(labels_true, predicted_labels)\n",
    "f1 = f1_score(labels_true, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efdef9-4b7a-45c4-8317-32247272ada5",
   "metadata": {},
   "source": [
    "## Comparison Model without LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54473560-3b11-44bd-bb2b-ff82ffeffcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load the pre-trained model (without fine-tuning)\n",
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a993c23-375f-49c4-8a2d-293a117d9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    \"This movie was fantastic! I loved it.\",\n",
    "    \"The film was boring and too long.\",\n",
    "    \"An average movie, nothing special.\",\n",
    "    \"I really enjoyed the story and acting.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d934a1b-7232-4554-8a69-8a30ab412993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'This movie was fantastic! I loved it.'\n",
      "Prediction: Negative (Confidence: 0.51)\n",
      "\n",
      "Text: 'The film was boring and too long.'\n",
      "Prediction: Negative (Confidence: 0.51)\n",
      "\n",
      "Text: 'An average movie, nothing special.'\n",
      "Prediction: Negative (Confidence: 0.52)\n",
      "\n",
      "Text: 'I really enjoyed the story and acting.'\n",
      "Prediction: Negative (Confidence: 0.53)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in sample_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = baseline_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        confidence, predicted_label_idx = torch.max(probs, dim=-1)\n",
    "        predicted_label = \"Positive\" if predicted_label_idx.item() == 1 else \"Negative\"\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Prediction: {predicted_label} (Confidence: {confidence.item():.2f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e8487-e7c2-4f05-a452-06b106160cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hf)",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
